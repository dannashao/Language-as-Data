{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright: Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4.2: Ngrams and Terminology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will take a closer look how to distinguish between words. We use the processed article from the previous lab. **Modify the code to work with all articles from your dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a688d2339954966b597042501de4177",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 12:06:00 INFO: Downloading default packages for language: fr (French) ...\n",
      "2023-11-06 12:06:01 INFO: File exists: /Users/lisabeinborn/stanza_resources/fr/default.zip\n",
      "2023-11-06 12:06:04 INFO: Finished downloading models and saved to /Users/lisabeinborn/stanza_resources.\n",
      "2023-11-06 12:06:04 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "795265c2911749d596efb66f889cb142",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 12:06:06 INFO: Loading these models for language: fr (French):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "| depparse  | combined_charlm   |\n",
      "| ner       | wikiner           |\n",
      "=================================\n",
      "\n",
      "2023-11-06 12:06:06 INFO: Using device: cpu\n",
      "2023-11-06 12:06:06 INFO: Loading: tokenize\n",
      "2023-11-06 12:06:06 INFO: Loading: mwt\n",
      "2023-11-06 12:06:06 INFO: Loading: pos\n",
      "2023-11-06 12:06:06 INFO: Loading: lemma\n",
      "2023-11-06 12:06:06 INFO: Loading: depparse\n",
      "2023-11-06 12:06:06 INFO: Loading: ner\n",
      "2023-11-06 12:06:07 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "import pandas as pd\n",
    "language = \"fr\"\n",
    "article_file = \"../data/veganism_overview_\" + language +\".tsv\"\n",
    "content = pd.read_csv(article_file, sep=\"\\t\", header = 0, keep_default_na=False)\n",
    "\n",
    "# Prepare the nlp pipeline\n",
    "stanza.download(language)\n",
    "nlp = stanza.Pipeline(language)\n",
    "\n",
    "current_article = content[\"Text\"][0]\n",
    "nlp_output = nlp(current_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, a sequence of several tokens should be interpreted as a compound or a fixed phrase: \n",
    "- New York\n",
    "- front door\n",
    "- Chief Executive Officer\n",
    "- kick the bucket\n",
    "- state of the art\n",
    "\n",
    "In order to account for frequent phrases, we can extract ngram statistics. **Try different values for n and analyze the results!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('noix de cajou', 3), ('de noix de', 2), ('et alors ?', 2), ('il y a', 2), ('indispensable à notre', 2), ('de la viande', 2), (\"d' origine animale\", 2), ('( de soja', 2), ('huile de cajou', 2), (', nous ne', 2), ('nous ne serions', 2), ('ne serions pas', 2), ('pour en parler', 2), ('Encore une minorité', 2), (\"une minorité d'\", 2), (\"minorité d' ayatollah\", 2), (\"d' ayatollah qui\", 2), ('pourrir la vie', 2), ('la vie de', 2), ('vie de la', 2)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "def calculate_ngram_frequencies(n, nlp_output):\n",
    "    ngram_frequencies = Counter()\n",
    "    for sentence in nlp_output.sentences:\n",
    "        tokens = [token.text for token in sentence.tokens]\n",
    "        ngrams = [\" \".join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "        ngram_frequencies.update(ngrams)\n",
    "    return ngram_frequencies\n",
    "n = 3\n",
    "ngram_frequencies = calculate_ngram_frequencies(n, nlp_output)\n",
    "print(ngram_frequencies.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most frequent words are stopwords. For some research questions, it might make sense to ignore the stopwords.\n",
    "\n",
    "**Search for the commonly used stopwords for your target language. Discuss how stopword removal affects the interpretation of the ngram statistics.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('noix cajou', 3), (') ,', 3), (', lait', 3), (', alors', 3), ('alors ?', 2), ('vegan ça', 2), ('plus vite', 2), (\", c'\", 2), ('origine animale', 2), ('( soja', 2), ('huile cajou', 2), ('etc ...', 2), ('chose ?', 2), (', parler', 2), ('Encore minorité', 2), ('minorité ayatollah', 2), ('pourrir vie', 2), ('vie majorité', 2), ('majorité ...', 2), ('Les véganeries', 1)]\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# These are the stopwords defined for French in the nltk module and I added the determiners \"d'\" and \"l'\"\n",
    "stopwords = ['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', \"d'\",'elle', 'en', 'et', 'eux', 'il', 'ils', 'je', 'la', 'le', 'les',\"l'\", 'leur', 'lui', 'ma', 'mais', 'me', 'même', 'mes', 'moi', 'mon', 'ne', 'nos', 'notre', 'nous', 'on', 'ou', 'par', 'pas', 'pour', 'qu', 'que', 'qui', 'sa', 'se', 'ses', 'son', 'sur', 'ta', 'te', 'tes', 'toi', 'ton', 'tu', 'un', 'une', 'vos', 'votre', 'vous', 'c', 'd', 'j', 'l', 'à', 'm', 'n', 's', 't', 'y', 'été', 'étée', 'étées', 'étés', 'étant', 'étante', 'étants', 'étantes', 'suis', 'es', 'est', 'sommes', 'êtes', 'sont', 'serai', 'seras', 'sera', 'serons', 'serez', 'seront', 'serais', 'serait', 'serions', 'seriez', 'seraient', 'étais', 'était', 'étions', 'étiez', 'étaient', 'fus', 'fut', 'fûmes', 'fûtes', 'furent', 'sois', 'soit', 'soyons', 'soyez', 'soient', 'fusse', 'fusses', 'fût', 'fussions', 'fussiez', 'fussent', 'ayant', 'ayante', 'ayantes', 'ayants', 'eu', 'eue', 'eues', 'eus', 'ai', 'as', 'avons', 'avez', 'ont', 'aurai', 'auras', 'aura', 'aurons', 'aurez', 'auront', 'aurais', 'aurait', 'aurions', 'auriez', 'auraient', 'avais', 'avait', 'avions', 'aviez', 'avaient', 'eut', 'eûmes', 'eûtes', 'eurent', 'aie', 'aies', 'ait', 'ayons', 'ayez', 'aient', 'eusse', 'eusses', 'eût', 'eussions', 'eussiez', 'eussent']\n",
    "\n",
    "def calculate_ngram_frequencies_without_stopwords(n, nlp_output):\n",
    "    ngram_frequencies = Counter()\n",
    "    for sentence in nlp_output.sentences:\n",
    "        # Here we remove stopwords, Note: take some time to understand the syntax of the list comprehension, it is not intuitive \n",
    "        tokens = [token.text for token in sentence.tokens if token.text not in stopwords ]\n",
    "\n",
    "        ngrams = [\" \".join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "        ngram_frequencies.update(ngrams)\n",
    "    return ngram_frequencies\n",
    "\n",
    "n = 2\n",
    "ngram_frequencies = calculate_ngram_frequencies_without_stopwords(n, nlp_output)\n",
    "print(ngram_frequencies.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to determine the relative importance of a term for an article, we can normalize its frequency by the frequency of the term in all articles. \n",
    "\n",
    "**Important: frequencies need to be calculated for the same ngram size.**\n",
    "\n",
    "The code currently distinguishes between uppercase and lowercase words. For many languages and tasks, it is useful to lowercase all words. **Think about the influence of casing on your research question.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mZeroDivisionError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[17], line 9\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m term, freq \u001B[38;5;129;01min\u001B[39;00m frequencies_currentarticle\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;66;03m# Remove stopwords and punctuation? --> experimental choice\u001B[39;00m\n\u001B[1;32m      8\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m term \u001B[38;5;129;01min\u001B[39;00m stopwords \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m term \u001B[38;5;129;01min\u001B[39;00m string\u001B[38;5;241m.\u001B[39mpunctuation:\n\u001B[0;32m----> 9\u001B[0m         normalized_frequency \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mfloat\u001B[39m(\u001B[43mfreq\u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43mfrequencies_dataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43mterm\u001B[49m\u001B[43m]\u001B[49m)\n\u001B[1;32m     10\u001B[0m         normalized_frequencies[term] \u001B[38;5;241m=\u001B[39m normalized_frequency\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28mprint\u001B[39m(normalized_frequencies\u001B[38;5;241m.\u001B[39mmost_common(\u001B[38;5;241m100\u001B[39m))\n",
      "\u001B[0;31mZeroDivisionError\u001B[0m: division by zero"
     ]
    }
   ],
   "source": [
    "frequencies_currentarticle = calculate_ngram_frequencies(1, nlp_output)\n",
    "# You calculated the document frequencies in an earlier lab\n",
    "frequencies_dataset = pickle.load(open(\"../data/processed_data/tokenfrequencies.pkl\",\"rb\"))\n",
    "\n",
    "normalized_frequencies = Counter()\n",
    "for term, freq in frequencies_currentarticle.items():\n",
    "    # Remove stopwords and punctuation? --> experimental choice\n",
    "    if not term in stopwords and not term in string.punctuation:\n",
    "        normalized_frequency = float(freq/frequencies_dataset[term])\n",
    "        normalized_frequencies[term] = normalized_frequency\n",
    "    \n",
    "print(normalized_frequencies.most_common(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code currently throws a *ZeroDivisionError*. **What does that mean and how can you fix it?** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4. Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stanza pipeline performs named entity recognition. On the token-level, a named entity label can be split into the position and the category.  **The named entity labels might vary across labels because they depend on the labels used in the training dataset. Check [the documentation](https://stanfordnlp.github.io/stanza/available_models.html) for your language (scroll down to NER).** We are trying out the English article here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef63f5a619ac4d5b92d1e65822baebbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 12:07:36 INFO: Downloading default packages for language: en (English) ...\n",
      "2023-11-06 12:07:37 INFO: File exists: /Users/lisabeinborn/stanza_resources/en/default.zip\n",
      "2023-11-06 12:07:39 INFO: Finished downloading models and saved to /Users/lisabeinborn/stanza_resources.\n",
      "2023-11-06 12:07:39 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b8f0fb820764ac18fca6e32a36b89af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 12:07:41 INFO: Loading these models for language: en (English):\n",
      "======================================\n",
      "| Processor    | Package             |\n",
      "--------------------------------------\n",
      "| tokenize     | combined            |\n",
      "| pos          | combined_charlm     |\n",
      "| lemma        | combined_nocharlm   |\n",
      "| constituency | ptb3-revised_charlm |\n",
      "| depparse     | combined_charlm     |\n",
      "| sentiment    | sstplus             |\n",
      "| ner          | ontonotes_charlm    |\n",
      "======================================\n",
      "\n",
      "2023-11-06 12:07:41 INFO: Using device: cpu\n",
      "2023-11-06 12:07:41 INFO: Loading: tokenize\n",
      "2023-11-06 12:07:41 INFO: Loading: pos\n",
      "2023-11-06 12:07:41 INFO: Loading: lemma\n",
      "2023-11-06 12:07:41 INFO: Loading: constituency\n",
      "2023-11-06 12:07:42 INFO: Loading: depparse\n",
      "2023-11-06 12:07:42 INFO: Loading: sentiment\n",
      "2023-11-06 12:07:42 INFO: Loading: ner\n",
      "2023-11-06 12:07:43 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "import pandas as pd\n",
    "language = \"en\"\n",
    "article_file = \"../data/veganism_overview_\" + language +\".tsv\"\n",
    "content = pd.read_csv(article_file, sep=\"\\t\", header = 0, keep_default_na=False)\n",
    "\n",
    "# Prepare the nlp pipeline\n",
    "stanza.download(language)\n",
    "nlp = stanza.Pipeline(language)\n",
    "\n",
    "current_article = content[\"Text\"][0]\n",
    "nlp_output = nlp(current_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Thirty years ago, a few Indian-Americans got together to form Vegetarian Vision when they saw more and more Indian immigrants becoming non-vegetarian of the difficulty accessing their traditional Indian products.\n",
      "\n",
      "B-DATE Thirty\n",
      "I-DATE years\n",
      "E-DATE ago\n",
      "Multi-token entity: DATE Thirty years ago\n",
      "----\n",
      "B-NORP Indian\n",
      "I-NORP -\n",
      "E-NORP Americans\n",
      "Multi-token entity: NORP Indian - Americans\n",
      "----\n",
      "B-ORG Vegetarian\n",
      "E-ORG Vision\n",
      "Multi-token entity: ORG Vegetarian Vision\n",
      "----\n",
      "S-NORP Indian\n",
      "Single-token entity: NORP Indian\n",
      "----\n",
      "S-NORP Indian\n",
      "Single-token entity: NORP Indian\n",
      "----\n",
      "\n",
      "“People coming from India couldn’t find enough vegetarian food.\n",
      "\n",
      "S-GPE India\n",
      "Single-token entity: GPE India\n",
      "----\n",
      "\n",
      "So they were changing their lifestyle.\n",
      "\n",
      "\n",
      "We felt an organization like this was needed,” Chairman H.K.\n",
      "\n",
      "S-PERSON H.K\n",
      "Single-token entity: PERSON H.K\n",
      "----\n",
      "\n",
      "Shah, founder of Vegetarian Vision founded in 1992, now called World Vegan Vision, told News India Times.\n",
      "\n",
      "S-PERSON Shah\n",
      "Single-token entity: PERSON Shah\n",
      "----\n",
      "B-ORG Vegetarian\n",
      "E-ORG Vision\n",
      "Multi-token entity: ORG Vegetarian Vision\n",
      "----\n",
      "S-DATE 1992\n",
      "Single-token entity: DATE 1992\n",
      "----\n",
      "B-ORG World\n",
      "I-ORG Vegan\n",
      "E-ORG Vision\n",
      "Multi-token entity: ORG World Vegan Vision\n",
      "----\n",
      "B-ORG News\n",
      "I-ORG India\n",
      "E-ORG Times\n",
      "Multi-token entity: ORG News India Times\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "sentences = nlp_output.sentences\n",
    "\n",
    "# For the example, I only look at the first three sentences. Make sure to change this.\n",
    "for sentence in sentences[0:5]:\n",
    "    print()\n",
    "    print(sentence.text)\n",
    "    print()\n",
    "    for token in sentence.tokens:\n",
    "        if not token.ner ==\"O\":\n",
    "\n",
    "            # This shows us the labels on the token level\n",
    "            print(token.ner, token.text)\n",
    "            position, category = token.ner.split(\"-\")\n",
    "\n",
    "            # Code to combine token labels into entity labels\n",
    "            if (position == \"S\"):\n",
    "                print(\"Single-token entity: \" + category, token.text)\n",
    "                print(\"----\")\n",
    "            if (position == \"B\"):\n",
    "                current_token = token.text\n",
    "            if (position == \"I\"):\n",
    "                current_token = current_token + \" \" + token.text\n",
    "            if (position == \"E\"):\n",
    "                current_token = current_token + \" \" + token.text\n",
    "                print(\"Multi-token entity: \" + category, current_token)\n",
    "                print(\"----\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATE Thirty years ago\n",
      "NORP Indian-Americans\n",
      "ORG Vegetarian Vision\n",
      "NORP Indian\n",
      "NORP Indian\n",
      "GPE India\n"
     ]
    }
   ],
   "source": [
    "# We can also directly access the entities\n",
    "for sentence in sentences[0:2]:\n",
    "    for entity in sentence.entities:\n",
    "        print(entity.type, entity.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Which role do named entities play for your dataset?**  How can you adjust the frequency calculations to make sure that named entities consisting of multiple words are treated as a single term? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LaD2023",
   "language": "python",
   "name": "lad2023"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}