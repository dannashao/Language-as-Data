{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright: Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4.3: Keywords and Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we learn how to cluster documents. The code is partially adapted from [this notebook](https://www.kaggle.com/cherishzhang/clustering-on-papers). We compare different ways to represent the keywords in documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tf-idf\n",
    "\n",
    "Calculating tf-idf (term frequency - inverse document frequency) is a simple approach to extract the key words of an article. The class TfidfVectorizer from the module sklearn calculates the tf-idf scores for all terms in our documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import stanza\n",
    "import string\n",
    "\n",
    "# This is very simplistic pre-processing. You might want to modify it\n",
    "def preprocess(article):\n",
    "    processed_article = nlp.process(article)\n",
    "    all_lemmas = []\n",
    "    for s in processed_article.sentences: \n",
    "        if len(s.text.strip())>0:\n",
    "            lemmas = [word.lemma.lower() for word in s.words if not word.lemma==None]\n",
    "            stopwords = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
    "            clean_lemmas = [lemma for lemma in lemmas if not lemma in stopwords and not lemma in string.punctuation]\n",
    "            all_lemmas.extend(clean_lemmas)\n",
    "    return all_lemmas\n",
    "\n",
    "# Read in TSV\n",
    "tsv_file = \"../data/veganism_overview_en.tsv\"\n",
    "news_content = pd.read_csv(tsv_file, sep=\"\\t\", keep_default_na=False, header=0)\n",
    "nlp = stanza.Pipeline('en', processors='tokenize,pos,lemma')\n",
    "\n",
    "# We filter out empty articles\n",
    "news_content = news_content[news_content[\"Text\"].str.len() >0 ]\n",
    "articles = news_content[\"Text\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# You can play around with the ngram range\n",
    "vectorizer = TfidfVectorizer(use_idf=True, tokenizer=preprocess)\n",
    "tf_idf = vectorizer.fit_transform(articles)\n",
    "all_terms = vectorizer.get_feature_names_out()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The terms are ordered alphabetically. **Do some spot checks and come up with ideas for better pre-processing of the articles.**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly look at some terms\n",
    "print(all_terms[0:50])\n",
    "\n",
    "# Select a document\n",
    "i = 3\n",
    "print(tf_idf[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectorizer.get_feature_names_out()[3891])\n",
    "print(vectorizer.get_feature_names_out()[2914])\n",
    "print(vectorizer.get_feature_names_out()[912])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clustering\n",
    "\n",
    "In clustering, we try to infer groups of similar documents. Here, we use the k-means algorithm of the *sklearn* module and the tf-idf vectors as document representation. The number of clusters is an experimental parameter. **Analyze the clusters you obtain. Do they correspond to useful conceptual groups? What happens if you vary the number of clusters?**\n",
    "\n",
    "Instead of clustering documents, you could also cluster sentences from multiple documents. This could result in argumentative clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many clusters do you expect? \n",
    "from sklearn.cluster import KMeans\n",
    "num_clusters = 4\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "km.fit(tf_idf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the clusters\n",
    "clusters = km.labels_.tolist()\n",
    "clustered_articles ={'Title': news_content[\"Title\"],'Author': news_content[\"Author\"],'Publisher': news_content[\"Publisher\"], 'Cluster': clusters}\n",
    "overview = pd.DataFrame(clustered_articles, columns = ['Author', 'Title', 'Publisher', 'Cluster'])\n",
    "overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Represent a document by keywords\n",
    "\n",
    "Instead of representing a document by all of its words, we could focus on the most relevant words. In this example, we extract the words with the highest tf-idf as keywords. **Do you think these are representative keywords? What could be improved?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# We extract the keywords\n",
    "num_keywords = 10\n",
    "\n",
    "def get_top_tfidf_features(row, terms, top_n=25):\n",
    "    top_ids = np.argsort(row)[::-1][:top_n]\n",
    "    top_features = [terms[i] for i in top_ids]\n",
    "    return top_features, top_ids\n",
    "\n",
    "keywords = []\n",
    "keyword_ids = []\n",
    "for i in range(0, tf_idf.shape[0]):\n",
    "    row = np.squeeze(tf_idf[i].toarray())\n",
    "    top_terms, top_ids= get_top_tfidf_features(row, all_terms, top_n=num_keywords)\n",
    "    keywords.append(top_terms)\n",
    "    keyword_ids.append(top_ids)\n",
    "# Show a few keywords\n",
    "for x in range(8):\n",
    "    print(\"Keywords for article \" + str(x))\n",
    "    print(keywords[x])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4. Represent keywords with vectors\n",
    "\n",
    "We could now calculate the clusters directly on the keyword ids as document representation (might be a good idea to try this out). This representation has two disadavantages: 1. the order of the keywords is taken into account by the clustering algorithm (e.g. keyword \"the\" on position 2 is not similar to \"the\" on position 4) and 2. the ids do not capture similarities between words.\n",
    "\n",
    "We now represent each keyword with a vector from a pre-trained embedding model (trained on Wikipedia) and then take the mean vector over all keywords. Loading the model takes time. We will learn more about word vectors in the next lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "print(\"loading\")\n",
    "fasttext_model  = KeyedVectors.load_word2vec_format(\"../data/wiki-news-300d-1M.vec\")\n",
    "print(\"done loading\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "all_doc_representations = []\n",
    "\n",
    "for doc_keywords in keywords:\n",
    "    doc_representation =[]\n",
    "    for keyword in doc_keywords:\n",
    "        try:\n",
    "            word_representation = fasttext_model[keyword]\n",
    "            doc_representation.append(word_representation)\n",
    "        except KeyError as e:\n",
    "            # We simply ignore unknown words\n",
    "            print(e)\n",
    "\n",
    "\n",
    "    # Take the mean over the keywords\n",
    "    mean_keywords = np.mean(doc_representation, axis=0)\n",
    "    all_doc_representations.append(mean_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Now, let's cluster on the mean keyword vector\n",
    "from sklearn.cluster import KMeans\n",
    "num_clusters = 4\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "km.fit(all_doc_representations)\n",
    "# Output the clusters\n",
    "clusters = km.labels_.tolist()\n",
    "clustered_articles ={'Title': news_content[\"Title\"],'Author': news_content[\"Author\"],'Publisher': news_content[\"Publisher\"], 'Cluster': clusters}\n",
    "overview = pd.DataFrame(clustered_articles, columns = ['Author', 'Title', 'Publisher', 'Cluster'])\n",
    "overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 5. Word clouds\n",
    "\n",
    "Word clouds are a way to visualize key words. They have lost in popularity recently, but can still provide a means for exploration when you want to investigate the quality of your clusters. The size of a word in the word cloud visualizes its frequency.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def wordcloud_cluster_byIds(clusterId, clusters, keywords):\n",
    "    words = []\n",
    "    for i in range(0, len(clusters)):\n",
    "        if clusters[i] == clusterId:\n",
    "            for word in keywords[i]:\n",
    "                words.append(word)\n",
    "    print(words)\n",
    "    # Generate a word cloud based on the frequency of the terms in the cluster\n",
    "    wordcloud = WordCloud(max_font_size=40, relative_scaling=.8).generate(' '.join(words))\n",
    "   \n",
    "    plt.figure()\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(str(clusterId)+\".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "wordcloud_cluster_byIds(3, clusters, keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Clustering by style\n",
    "Instead of clustering documents based on their content, you could also cluster documents based on the stylistic features you extracted in lab 3. **Try it out!**\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Clustering by style\n",
    "Instead of clustering documents based on their content, you could also cluster documents based on the stylistic features you extracted in lab 3. **Try it out!**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LaD2023",
   "language": "python",
   "name": "lad2023"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}