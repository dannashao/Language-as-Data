{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1046fe5e",
   "metadata": {},
   "source": [
    "Copyright: Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03ffe65",
   "metadata": {},
   "source": [
    "# Lab 1.3: Extracting Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44192b98",
   "metadata": {},
   "source": [
    "In this notebook, we use the [https://mediastack.com/ ](MediaStack) to search for news: \n",
    "\n",
    "We learn how to extract some metadata from the API. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cd7a8e",
   "metadata": {},
   "source": [
    "Go to https://mediastack.com/signup and fill in the required information. <br>\n",
    "Save the *API Access Key* and use it in the code. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2729d396",
   "metadata": {},
   "source": [
    "## 1. Queries in different languages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb13fa91",
   "metadata": {},
   "source": [
    "In the Media Stack API, we can vary different parameters, such as the keywords and the language.\n",
    "\n",
    "The language needs to be abbreviated according to the two-letter ISO-639-1 code.\n",
    "\n",
    "Try out different queries and languages.\n",
    "\n",
    "**Language codes**: There are different ISO code classifications for languages. ISO-639-1 is the oldest one and uses two letters. More recent schemes use three letters to include more languages (living and extinct):\n",
    "\n",
    "https://www.iso.org/iso-639-language-codes.html\n",
    "https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes\n",
    "\n",
    "**Query limit**: Mediastack only allows you to access maximum 100 articles each query. You can get around this limit by changing the parameters, e.g. offset, source, category with every query. Check the [https://mediastack.com/documentation](documentation).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ad12d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import http.client, urllib.parse, json\n",
    "from util_html import *\n",
    "\n",
    "conn = http.client.HTTPConnection('api.mediastack.com')\n",
    "\n",
    "params = urllib.parse.urlencode({\n",
    "    'access_key': '', ## ADD YOUR ACCESS KEY\n",
    "    'keywords': 'vegan',\n",
    "    'sort': 'published_desc',\n",
    "    'languages':'en', \n",
    "    'limit': 100\n",
    "    })\n",
    "\n",
    "conn.request('GET', '/v1/news?{}'.format(params))\n",
    "\n",
    "res = conn.getresponse()\n",
    "data = res.read()\n",
    "\n",
    "query_content=(data.decode('utf-8'))\n",
    "\n",
    "query = json.loads(query_content)\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4233c577",
   "metadata": {},
   "source": [
    "# 2. Extracting Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ece772",
   "metadata": {},
   "source": [
    "Media Stack lists many articles for each query. As with the NOS-articles, we first want to extract the links. If you click on the links, you will notice that Media Stack does not write own articles, but just lists articles from other sources. In the following function, we try to extract metadata from the html. \n",
    "\n",
    "This particular strategy for metadata extraction only works for this version of Media Stack.\n",
    "If you use another engine or if their code changes, you will need to adapt the metadata extraction. \n",
    "\n",
    "**Make sure to add additional printouts to inspect the html content and understand how we find the metadata.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a695ddf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_metadata(article):\n",
    "    # Extract the publication date\n",
    "    published_at = article['published_at']\n",
    "    if published_at:\n",
    "        date, time = published_at.split(\"T\")        \n",
    "    else:\n",
    "        date = \"\"\n",
    "        time = \"\"\n",
    "\n",
    "    # Extract meta data\n",
    "    url = article ['url']\n",
    "    title= article['title'] \n",
    "    \n",
    "    # category associated with the given news article\n",
    "    category = article['category']\n",
    "    \n",
    "    # country code associated with given article \n",
    "    country = article ['country']\n",
    "    \n",
    "    return date, time, title, url, category, country\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bc64cd",
   "metadata": {},
   "source": [
    "# 3 Extracting Content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759a30e7",
   "metadata": {},
   "source": [
    "In util_html.py, you find two additional functions: *parse_author* and *parse_news_text*. These functions try to extract the author and the text from each article. Note, that the functions are only approximations. They might fail because we do not know the html structure of every publisher. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd741a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = query[\"data\"]\n",
    "max = 10\n",
    "for i, article in enumerate(articles):\n",
    "    if i < max: \n",
    "        \n",
    "        date, time, title, article_url, category, country = extract_metadata(article)\n",
    "    \n",
    "\n",
    "        article_content = url_to_html(article_url)   \n",
    "        author = parse_author(article_content)\n",
    "        content = parse_news_text(article_content)\n",
    "        \n",
    "        print(date, time)\n",
    "        print(article_url)\n",
    "        print(\"author:\", author)  \n",
    "        print(\"title:\", title) \n",
    "        print(\"category:\",category)\n",
    "        print(\"country:\",country)\n",
    "        print(content[:100])\n",
    "        print()\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9f2a90",
   "metadata": {},
   "source": [
    "# 4. Saving results as TSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37710e68",
   "metadata": {},
   "source": [
    "A standardized open format for storing the content of multiple variables are CSV files. CSV stands for comma-separated values. CSV files are text-based, but when they are imported to a spreadsheet program such as Excel, they are displayed as a table. Lines in the text file are interpreted as rows; commas in the text file are interpreted as separators for columns.  \n",
    "\n",
    "Most programmers prefer to use TSV files. In these files, the values are separated by tabulators (\"\\t\") instead of commas. Both variants can be easily processed, but you need to know which separator has been used. \n",
    "\n",
    "**If necessary, recap information on CSV and TSV files in [Chapter 16](https://github.com/cltl/python-for-text-analysis/blob/master/Chapters/Chapter%2016%20-%20Data%20formats%20I%20(CSV%20and%20TSV).ipynb) of the python course.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4767b482",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = http.client.HTTPConnection('api.mediastack.com')\n",
    "\n",
    "keywords = 'veganism'\n",
    "\n",
    "params = urllib.parse.urlencode({\n",
    "    'access_key': '',## YOUR ACCESS KEY\n",
    "    'keywords' : keywords,\n",
    "    'sort': 'published_desc',\n",
    "    'languages':'en', \n",
    "    'limit': 100\n",
    "    })\n",
    "\n",
    "conn.request('GET', '/v1/news?{}'.format(params))\n",
    "\n",
    "res = conn.getresponse()\n",
    "data = res.read()\n",
    "\n",
    "query_content=(data.decode('utf-8'))\n",
    "query = json.loads(query_content)\n",
    "\n",
    "outfile = \"../results/mediastack_results/\" + keywords +\"_overview.tsv\"\n",
    "\n",
    "with open(outfile, \"w\",encoding=\"utf-8\") as f:\n",
    "    date, time, title, article_url, category, country = extract_metadata(article)\n",
    "\n",
    "    f.write(\"Publication Date\\tTime\\tAuthor\\tTitle\\tURL\\tText\\n\")\n",
    "    \n",
    "    for i, article in enumerate(articles):\n",
    "        \n",
    "        # Extract metadata\n",
    "        date, time, title, article_url, category, country = extract_metadata(article)\n",
    "        \n",
    "        # Extract content\n",
    "        article_content = url_to_html(article_url)\n",
    "        author = parse_author(article_content)\n",
    "        content = parse_news_text(article_content)\n",
    "        \n",
    "        # We remove the newlines from the content, so that we can easily store it in a single line. \n",
    "        # Keep in mind, that newlines can also carry meaning.\n",
    "        # For example, they separate paragraphs and this information is lost in the analysis, if we remove them. \n",
    "        content = content.replace(\"\\n\", \"\")\n",
    "        \n",
    "        # We want the fields to be separated by tabulators (\\t)\n",
    "        output = \"\\t\".join([date, time, author, title, article_url, content])\n",
    "        f.write(output +\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59796e3f",
   "metadata": {},
   "source": [
    "# 5. Inspect the results.\n",
    "\n",
    "You will notice that we do not always find a value for the author. There can be two reasons for that: \n",
    "- The author name is not provided by the publisher.\n",
    "- Our code cannot find it.\n",
    "\n",
    "Double-check on the website which explanation holds. When we are working with automatic methods, we will always be confronted with the issue of missing data. \n",
    "\n",
    "**Discuss how this can affect the methodology and interpretation of your experiments.** "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LaDKernel",
   "language": "python",
   "name": "ladkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
