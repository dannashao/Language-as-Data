{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright: Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1.4: Extracting Publications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ACL anthology is the most relevant resource for research publications on natural language processing.\n",
    "\n",
    "**Take a look at its search options in the browser:**\n",
    "https://www.aclweb.org/anthology/search/?q=opinion+mining\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Querying bibtex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Publications are commonly stored as bibtex-files. In this notebook, we work with a small subset of the anthology (the first 20,000 lines): anthology_small.bib\n",
    "\n",
    "**Inspect the file and make sure you understand the structure.**\n",
    "\n",
    "To get better results for your queries, download the full anthology from https://www.aclweb.org/anthology/anthology.bib.gz and extract it to your LaD/Lab1 folder. \n",
    "\n",
    "Let's load the file and parse it using bibtexparser (this takes a moment): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading...\n",
      "Done.\n",
      "Number of articles: 1236\n"
     ]
    }
   ],
   "source": [
    "import bibtexparser\n",
    "\n",
    "with open(\"../data/anthology_small.bib\") as bibtex_file:\n",
    "    # Parse the bibtex file - this takes a while\n",
    "    parser = bibtexparser.bparser.BibTexParser(common_strings=True)\n",
    "    print(\"Loading...\")\n",
    "    anthology= bibtexparser.load(bibtex_file, parser)\n",
    "    print(\"Done.\")\n",
    "    # Only choose entries with the type \"inproceedings\"\n",
    "    articles = [article for article in anthology.entries if article[\"ENTRYTYPE\"]==\"inproceedings\"]\n",
    "    \n",
    "    print(\"Number of articles: \" + str(len(articles)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Saving results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now query the articles and collect all urls of interesting articles in a list. We save both the abstracts and the full pdfs. \n",
    "\n",
    "**Try out different queries. Add code to filter by author, year, or booktitle. You can also modify the code to use regular expressions as queries.**\n",
    "\n",
    "Note that the error : \"Not Acceptable! An appropriate representation of the requested resource could not be found on this server. This error was generated by Mod_Security\" may occur. This happens if cookies are turned off and mod_security requires cookies to match session data. Instead of seeing the website, we only see this error. (https://stackoverflow.com/questions/28090737/not-acceptable-an-appropriate-representation-of-the-requested-resource-could-no)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kumar-etal-2020-evaluating evaluating aggression identification in social media\n",
      "evaluating aggression identification in social media https://www.aclweb.org/anthology/2020.trac-1.1\n",
      "\n",
      "ramiandrisoa-mothe-2020-aggression aggression identification in social media: a transfer learning based approach\n",
      "aggression identification in social media: a transfer learning based approach https://www.aclweb.org/anthology/2020.trac-1.5\n",
      "\n",
      "pascucci-etal-2020-role the role of computational stylometry in identifying (misogynistic) aggression in english social media texts\n",
      "the role of computational stylometry in identifying (misogynistic) aggression in english social media texts https://www.aclweb.org/anthology/2020.trac-1.11\n",
      "\n",
      "diaz-torres-etal-2020-automatic automatic detection of offensive language in social media: defining linguistic criteria to build a mexican spanish dataset\n",
      "automatic detection of offensive language in social media: defining linguistic criteria to build a mexican spanish dataset https://www.aclweb.org/anthology/2020.trac-1.21\n",
      "\n",
      "safi-samghabadi-etal-2020-detecting detecting early signs of cyberbullying in social media\n",
      "detecting early signs of cyberbullying in social media https://www.aclweb.org/anthology/2020.trac-1.23\n",
      "\n",
      "abeywardana-thayasivam-2020-privacy a privacy preserving data publishing middleware for unstructured, textual social media data\n",
      "a privacy preserving data publishing middleware for unstructured, textual social media data https://www.aclweb.org/anthology/2020.stoc-1.4\n",
      "\n",
      "jia-etal-2020-incorporating incorporating uncertain segmentation information into chinese ner for social media text\n",
      "incorporating uncertain segmentation information into chinese ner for social media text https://www.aclweb.org/anthology/2020.socialnlp-1.7\n",
      "\n",
      "saroj-pal-2020-indian an indian language social media collection for hate and offensive speech\n",
      "an indian language social media collection for hate and offensive speech https://www.aclweb.org/anthology/2020.restup-1.2\n",
      "\n",
      "abdul-mageed-etal-2020-aranet aranet: a deep learning toolkit for arabic social media\n",
      "aranet: a deep learning toolkit for arabic social media https://www.aclweb.org/anthology/2020.osact-1.3\n",
      "\n",
      "alshehri-etal-2020-understanding understanding and detecting dangerous speech in social media\n",
      "understanding and detecting dangerous speech in social media https://www.aclweb.org/anthology/2020.osact-1.6\n",
      "\n",
      "straton-etal-2020-stigma stigma annotation scheme and stigmatized language detection in health-care discussions on social media\n",
      "stigma annotation scheme and stigmatized language detection in health-care discussions on social media https://www.aclweb.org/anthology/2020.lrec-1.148\n",
      "\n",
      "lee-lau-2020-event an event-comment social media corpus for implicit emotion analysis\n",
      "an event-comment social media corpus for implicit emotion analysis https://www.aclweb.org/anthology/2020.lrec-1.203\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from util_html import url_to_html\n",
    "\n",
    "query = \"social media\"\n",
    "\n",
    "pdf_path = \"../results/acl_results/pdf/\"\n",
    "abstracts_path = \"../results/acl_results/abstracts/\"\n",
    "\n",
    "\n",
    "#Some servers request to add a user-agent to the query: \n",
    "headers = requests.utils.default_headers()\n",
    "headers.update(\n",
    "    {\n",
    "        'User-Agent': 'My User Agent 1.0',\n",
    "    }\n",
    ")\n",
    "# headers = {\n",
    "#     'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36',\n",
    "# }\n",
    "\n",
    "for entry in articles:\n",
    "    # Some titles contain curly braces to indicate uppercase names. We remove them from the title. \n",
    "    title = entry[\"title\"].lower()\n",
    "    title = title.replace(\"{\",\"\")\n",
    "    title = title.replace(\"}\",\"\")\n",
    "\n",
    "    # Get target articles\n",
    "    if query in title:\n",
    "        try:\n",
    "            \n",
    "            # Get metadata \n",
    "            id = entry[\"ID\"]\n",
    "            author = entry[\"author\"]\n",
    "            author = author.replace(\"\\n\",\" \")\n",
    "            author = author.replace(\"  \",\" \")\n",
    "\n",
    "\n",
    "            # Save abstract\n",
    "            url = entry[\"url\"]          \n",
    "            response = url_to_html(url)\n",
    "            abstract = response.find(attrs={'class': 'card-body acl-abstract'}).text\n",
    "            abstract = abstract.replace(\"Abstract\", \"\",1)\n",
    "\n",
    "            with open(abstracts_path + id + \".txt\", 'w') as f:\n",
    "                f.write(author+\"\\n\"+title+\"\\n\"+abstract)\n",
    "\n",
    "            # Save pdf\n",
    "            pdf_response = requests.get(url + \".pdf\", headers = headers)\n",
    "            with open(pdf_path + id + \".pdf\", 'wb') as f:\n",
    "                f.write(pdf_response.content)\n",
    "\n",
    "            print(id, title)\n",
    "            print(title, url)\n",
    "            print()\n",
    "\n",
    "        # Ignore entries that do not contain a url\n",
    "        except KeyError as e:\n",
    "            #print(\"Entry does not have URL\")\n",
    "            pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extracting PDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to run our analyses on the full texts. However, it is not easy to extract texts from pdfs if you do not want to buy commercial software. \n",
    "\n",
    "Try out the code for extracting texts from pdfs below. For the moment, you can ignore the warnings. Currently, the code outputs only a part of the first file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pdfminer.converter:undefined: <PDFType1Font: basefont='AQKOUH+CMSY8'>, 13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../results/acl_results/pdf/abeywardana-thayasivam-2020-privacy.pdf\n",
      "A Privacy Preserving Data Publishing Middleware for Unstructured, Textual \n",
      "Social Media Data \n",
      "Prasadi Abeywardana, Uthayasanker Thayasivam \n",
      "Department of Computer Science and Engineering \n",
      "University of Moratuwa, Sri Lanka  \n",
      "prasadiapsara.18@cse.mrt.ac.lk, rtuthaya@cse.mrt.ac.lk  \n",
      "Abstract \n",
      "Privacy is going to be an integral part of data science and analytics in the coming years. The next hype of data experimentat ion is going \n",
      "to be heavily dependent on privacy preserving techniques mainly as it’s going to be a legal responsibility rather than a mere social \n",
      "responsibility. Privacy preservation becomes more challenging specially in the context of unstructured data. Social networks have \n",
      "become predominantly popular over the past couple of decades and they are creating a huge data lake at a high velocity. Social media \n",
      "profiles contain a wealth of personal and sensitive information, creating enormous opportunities for third parties to analyze them with \n",
      "different algorithms, draw conclusions and use in disinformation campaigns and micro targeting based dark advertising. This s tudy \n",
      "provides a mitigation mechanism for disinformation campaigns that are done based on the insights extracted from personal/sen sitive \n",
      "data analysis. Specifically, this research is aimed at building a privacy preserving data publishing middleware for unstructured social \n",
      "media data without compromising the true analytical value of those data. A novel way is proposed to apply traditional structured privacy \n",
      "preserving techniques on unstructured data. Creating a comprehensive twitter corpus annotated with privacy attributes is another \n",
      "objective of this research, especially because the research community is lacking one. \n",
      "Keywords: Privacy Preserving Data Mining, Privacy Preserving Data Publishing , Disinformation, Micro-targeting, Anonymization, \n",
      "Data Utility, Social Networks, Data Science\n",
      "1. Introduction \n",
      "Big data being a buzz word which has created an immense \n",
      "hype in the society, many analytical models are employed \n",
      "in order to repurpose those data and derive insights. With \n",
      "the advancements of distributed systems and theoretically \n",
      "cheap storage, there are less constraints to capture data as \n",
      "much as possible and store them. Collection of data related \n",
      "to individuals in a global scale has become mainstream \n",
      "because of this.  \n",
      "Data are collected in big scales and published to be used by \n",
      "different parties for different purposes. At this point of \n",
      "publishing, there should be a proper insurance for personal \n",
      "data, as the publishing party cannot guarantee for which \n",
      "purposes this personal information will be used by the \n",
      "utilizing party. \n",
      "Micro-targeting based on the third-party analysis done on \n",
      "personal data is used as a means of disinformation \n",
      "campaigns. A \n",
      "famous example \n",
      "for \n",
      "this \n",
      "is dark \n",
      "advertisements \n",
      "targeting specific users \n",
      "in a very \n",
      "personalized manner for sharing misinformation \n",
      "in \n",
      "political campaigns. This is achieved by identifying target \n",
      "users by analyzing their political preferences and showing \n",
      "them personalized dark ads with content they are highly \n",
      "likely to believe. Analyzing sensitive personal information \n",
      "and using them for various intentions without user consents \n",
      "makes it a combination of an ethical and legal concern \n",
      "(Alaphilippe et al., 2019).   \n",
      "1.1 Data Protection Regulations \n",
      "Until recently, privacy was just a social responsibility, but \n",
      "it’s no more like that, because many legal systems have \n",
      "begun to enforce laws on protecting individuals’ privacy. \n",
      "Specially incidents like what happened between Facebook \n",
      "and Cambridge Analytica have forced the governments and \n",
      "policy makers to look at personal information protection as \n",
      "an emerging concern. Following are some of such novel \n",
      "legal requirements which arouse recently. \n",
      "1.1.1 General Data Protection Regulation (GDPR) \n",
      "This is a regulation imposed by European Union (EU) on \n",
      "data protection and privacy for all individuals within the \n",
      "EU and the European Economic Area (EEA) (Wikipedia, \n",
      "2016). This is applicable to exporting and processing \n",
      "personal data in a region outside EU as well. The intention \n",
      "of this regulation is to make it easy for non -European \n",
      "companies to work with European bodies without any data \n",
      "breaches. \n",
      "1.1.2 Russian Federal Law on Personal Data \n",
      "This is a regulation which emphasizes on systemizing the \n",
      "data processing of individuals in Russia. This emphasizes \n",
      "on localizing personal data of Russian citizens to Russia \n",
      "(KPMG, 2018). \n",
      "1.1.3 German Bundesdatenschutzgesetz (BDSG) \n",
      "This governs the exposure of personal data, which are \n",
      "manually processed or stored in IT systems. This was being \n",
      "modified with certain amendments for a long period of time \n",
      "and has become stricter in the recent past. \n",
      "1.2 Social Threats of Personal Data Analysis \n",
      "Personal data are coming into analytical systems through \n",
      "various domains. Mobile data, health care data, social \n",
      "media data and web usage data are a few such domains \n",
      "which can pump a huge amount of personal data into \n",
      "analytical systems without the knowledge or consent of \n",
      "individuals. There’s one prominent area, which has \n",
      "reformed the sharing of personal information, that is none \n",
      "other than social media. People choose to share many \n",
      "information about themselves as well as their close ones, \n",
      "compromising the privacy of both parties (Mehta and Rao, \n",
      "2015). \n",
      "Social platforms offer their data to third parties and \n",
      "advertisers to use in their analysis and campaigns. But \n",
      "sometimes \n",
      "these data are used \n",
      "in micro \n",
      "targeted \n",
      "disinformation campaigns to share dark ads. These highly \n",
      "personalized adverts are heavily used in political contexts \n",
      " \n",
      "to influence voters by sharing misinformation. In order to \n",
      "host micro targeted ad campaigns, a lot of information \n",
      "related to individuals, their preferences and personality are \n",
      "required, and social media undoubtedly contain a fortune \n",
      "of such data. In the recent incident that involved Facebook, \n",
      "Cambridge Analytica and Global Science Research (GSR), \n",
      "millions of US Facebook users’ data were analyzed without \n",
      "their consent and used in voter targeting, which is unethical \n",
      "as it sounds (Alaphilippe et al., 2019). A solution to these \n",
      "concerns might be a law enforced privacy prese rving \n",
      "middleware that has to be adopted by any social media \n",
      "platform, before publishing their data to a third party. \n",
      "The purpose of this research is to come up with a \n",
      "framework to sanitize data and preserve privacy, which can \n",
      "be utilized before publishing textual social media data to \n",
      "any analytical 3rd party. This will ensure that any sensitive \n",
      "personal data will not be used in a way where a person’s \n",
      "identity is revealed, and the individuals will not be \n",
      "subjected to disinformation campaigns. Specifically, this \n",
      "research addresses the problem of sanitizing social media \n",
      "data, which becomes more challenging due to their \n",
      "unstructured nature. Twitter is used as the selected social \n",
      "media platform to train and evaluate the capabilities of this \n",
      "framework. A corpus of 3000 tweets is built and annotated \n",
      "to be used in the model training process. \n",
      "The rest of the paper is organized as follows. Some \n",
      "theoretical concepts related to privacy preserving data \n",
      "publishing particularly in the context of unstructured data \n",
      "will be discussed in the background section. Then the \n",
      "methodology adapted will be described followed by a \n",
      "section dedicated towards the dataset. Next section is about \n",
      "the experimental design and the results and after that a \n",
      "section is contributed for discussion and future work. \n",
      "Finally, the paper is concluded with a conclusion section. \n",
      "2. Background \n",
      "Publishing sensitive data related to individuals in a way that \n",
      "protects their privacy was a topic of interest for some time \n",
      "and many \n",
      "techniques are \n",
      "implemented with \n",
      "the \n",
      "contribution from various fields such as computer science, \n",
      "statistics and social science. A few theorical concepts from \n",
      "the PPDP domain are described under this section. \n",
      "2.1 Different Types of Attributes Related to \n",
      "Personal Data \n",
      "Attributes related to personal data can be classified as \n",
      "follows based on how they can identify an individual. \n",
      "These attributes are extracted and used in PPDP techniques \n",
      "(Mehta and Rao, 2015). \n",
      "2.1.1 \n",
      "Personal Information Identifiers \n",
      "These are the attributes such as ID, name or email address \n",
      "that can be directly used to identify an individual. These \n",
      "attributes uniquely recognize individuals from others. \n",
      "2.1.2 Quasi Identifiers \n",
      "These are the attributes that can be combined with other \n",
      "external data and used to identify an individual. For \n",
      "instance, age, gender, profession, race, religion can be \n",
      "considered as quasi identifiers. These are not unique \n",
      "identifiers by themselves but can be combined with another \n",
      "set of quasi identifiers to uniquely recognize a person. \n",
      "2.1.3 \n",
      "Sensitive Attributes \n",
      "These are the attributes that individuals do not want to \n",
      "reveal about \n",
      "themselves. Examples can be salary, \n",
      "relationship statuses and diseases. \n",
      "2.1.4 Non-Sensitive Attributes \n",
      "These are the attributes other than the above mentioned 3 \n",
      "types. They may not have a direct or indirect relationship \n",
      "to identify individuals. \n",
      "Any PPDP process should include a mechanism to identify \n",
      "these attributes related to personal data before applying any \n",
      "sanitization technique. Based on the nature of the attribute, \n",
      "different sanitization techniques must be applied. \n",
      "2.2 Existing Data Sanitization Techniques \n",
      "Many research works have been carried out to come up \n",
      "with various sanitization techniques to protect personal \n",
      "data (Mehta and Rao, 2015; Fung et al., 2010) \n",
      "2.2.1 \n",
      "Suppression \n",
      "This mechanism replaces some attribute values by a \n",
      "symbol like ‘*’ to indicate those attributes are repressed. \n",
      "For instance, a credit card number can be suppressed as \n",
      "34** **** ****. \n",
      "2.2.2 Generalization \n",
      "This implies replacing an attribute with a generalized value \n",
      "of its class, for instance male and female values of the \n",
      "gender attribute or a nationality at\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from util_pdf import convert_pdf_to_txt\n",
    "import os\n",
    "\n",
    "for pdf_file in os.listdir(pdf_path):\n",
    "    pdf = os.path.join(pdf_path,pdf_file)\n",
    "    print(pdf)\n",
    "    text = convert_pdf_to_txt(pdf)\n",
    "    print(text[0:10000])\n",
    "    print(\"\\n\\n\")\n",
    "    break\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove the \"break\" command and save the files as txt-files instead. Inspect the quality and discuss for which tasks this quality could be used.**\n",
    "\n",
    "One of the articles throws an error. What could be the reason?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
