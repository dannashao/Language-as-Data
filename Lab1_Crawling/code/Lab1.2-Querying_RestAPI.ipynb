{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright: Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1.2: Querying a REST API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "News articles are a powerful resource to analyze current trends and events. In this Lab, we will extract Dutch news articles from nos.nl.\n",
    "\n",
    "**Visit https://nos.nl/zoeken/ and have a look at the search utility.**\n",
    "\n",
    "It provides access to a database. Instead of searching the database in the browser, we can also access it using code through a so-called REST API. REST stands for REpresentational State Transfer and makes it possible to query information from a server. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Querying keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to create a dataset that contains articles related to a specific topic. In order to find these articles, we need to determine good keywords for the topic. In this example, we use the keyword \"veganisme\".\n",
    "**Make sure to test several keywords for your topic and inspect the quality of the results.**  \n",
    "\n",
    "Instead of manually copying all search results into files, we want to automate the task. We create a URL by appending the keyword to the search URL of NOS. Note that other websites can use a different syntax. It helps to first test the search interface manually to understand how searches are specified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from util_html import *\n",
    "\n",
    "keyword=\"veganisme\"\n",
    "url = \"https://nos.nl/zoeken/?q=\" + keyword\n",
    "\n",
    "print('The search request URL:', url)\n",
    "\n",
    "parser_content= url_to_html(url)\n",
    "\n",
    "# The class for the search results has a weird name\n",
    "# You can find it out when you look at the HTML source in your web browser\n",
    "search_results = parser_content.find_all(\"a\", {\"class\":\"sc-3bf07e7a-4 cyMYml\"})\n",
    "\n",
    "# For comparison, print the full output, scroll through it and make sure you find the search results in there. \n",
    "# print(parser_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Collecting results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you inspect the search results, you can see that they have different properties such as *title*, *time*, and *category*. It would be possible, to filter our results based on these categories. For the moment, we only want to collect the links to the articles in a list.  \n",
    "\n",
    "We iterate through all links, send a request and extract the text. Then we store the text in a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "domain = \"https://nos.nl/\"\n",
    "for i, link in enumerate(search_results):    \n",
    "    found_url = domain + link[\"href\"]\n",
    "    print(i, found_url)\n",
    "    \n",
    "    # Extract text and add the url as first line\n",
    "    text = found_url + '\\n'+ url_to_string(found_url) \n",
    "    \n",
    "    # Save in file\n",
    "    dir = \"../results/nos_search_results/\"\n",
    "    filename = keyword + \"_\" + str(i) + \".txt\"\n",
    "    with open(dir + filename, \"w\", encoding = \"utf-8\") as f:\n",
    "        f.write(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inspecting data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you collected your preliminary data, the most important pre-processing phase begins: **Data Analyis**. \n",
    "\n",
    "Inspect the collected files and examine the following aspects: \n",
    "- Are the articles related to the topic that you had in mind when choosing the keyword? Or do you need to re-adjust the query? Many search engines allow you to use combinations of keywords using regular expressions in your queries.\n",
    "- Is the data structure of the results homogeneous?\n",
    "- How useful is the extracted text? What kind of additional data pre-processing would be helpful? How could you do that? \n",
    "- Can you identify different thematic groups or perspectives in the results? Are the differences reflected in the language? \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try out other public APIs to collect texts from interesting sources. You can find some examples here: https://github.com/public-apis/public-apis. Experiment with data from different languages.** \n",
    "\n",
    "How do you need to adjust the code? \n",
    "\n",
    "Note: Many APIs require that you register and obtain an API Key. This key is an identifier that you add to the query. It controls the number of queries that you send to avoid abuse.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LaDKernel",
   "language": "python",
   "name": "ladkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
