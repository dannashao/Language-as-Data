{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright: Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1.4: Extracting Publications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ACL anthology is the most relevant resource for research publications on natural language processing.\n",
    "\n",
    "**Take a look at its search options in the browser:**\n",
    "https://www.aclweb.org/anthology/search/?q=opinion+mining\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Querying bibtex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Publications are commonly stored as bibtex-files. In this notebook, we work with a small subset of the anthology (the first 20,000 lines): anthology_small.bib\n",
    "\n",
    "**Inspect the file and make sure you understand the structure.**\n",
    "\n",
    "To get better results for your queries, download the full anthology from https://www.aclweb.org/anthology/anthology.bib.gz and extract it to your LaD/Lab1 folder. \n",
    "\n",
    "Let's load the file and parse it using bibtexparser (this takes a moment): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bibtexparser\n",
    "\n",
    "with open(\"../data/anthology_small.bib\") as bibtex_file:\n",
    "    # Parse the bibtex file - this takes a while\n",
    "    parser = bibtexparser.bparser.BibTexParser(common_strings=True)\n",
    "    print(\"Loading...\")\n",
    "    anthology= bibtexparser.load(bibtex_file, parser)\n",
    "    print(\"Done.\")\n",
    "    # Only choose entries with the type \"inproceedings\"\n",
    "    articles = [article for article in anthology.entries if article[\"ENTRYTYPE\"]==\"inproceedings\"]\n",
    "    \n",
    "    print(\"Number of articles: \" + str(len(articles)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Saving results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now query the articles and collect all urls of interesting articles in a list. We save both the abstracts and the full pdfs. \n",
    "\n",
    "**Try out different queries. Add code to filter by author, year, or booktitle. You can also modify the code to use regular expressions as queries.**\n",
    "\n",
    "Note that the error : \"Not Acceptable! An appropriate representation of the requested resource could not be found on this server. This error was generated by Mod_Security\" may occur. This happens if cookies are turned off and mod_security requires cookies to match session data. Instead of seeing the website, we only see this error. (https://stackoverflow.com/questions/28090737/not-acceptable-an-appropriate-representation-of-the-requested-resource-could-no)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from util_html import url_to_html\n",
    "\n",
    "query = \"social media\"\n",
    "\n",
    "pdf_path = \"../results/acl_results/pdf/\"\n",
    "abstracts_path = \"../results/acl_results/abstracts/\"\n",
    "\n",
    "\n",
    "#Some servers request to add a user-agent to the query: \n",
    "headers = requests.utils.default_headers()\n",
    "headers.update(\n",
    "    {\n",
    "        'User-Agent': 'My User Agent 1.0',\n",
    "    }\n",
    ")\n",
    "# headers = {\n",
    "#     'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36',\n",
    "# }\n",
    "\n",
    "for entry in articles:\n",
    "    # Some titles contain curly braces to indicate uppercase names. We remove them from the title. \n",
    "    title = entry[\"title\"].lower()\n",
    "    title = title.replace(\"{\",\"\")\n",
    "    title = title.replace(\"}\",\"\")\n",
    "\n",
    "    # Get target articles\n",
    "    if query in title:\n",
    "        try:\n",
    "            \n",
    "            # Get metadata \n",
    "            id = entry[\"ID\"]\n",
    "            author = entry[\"author\"]\n",
    "            author = author.replace(\"\\n\",\" \")\n",
    "            author = author.replace(\"  \",\" \")\n",
    "\n",
    "\n",
    "            # Save abstract\n",
    "            url = entry[\"url\"]          \n",
    "            response = url_to_html(url)\n",
    "            abstract = response.find(attrs={'class': 'card-body acl-abstract'}).text\n",
    "            abstract = abstract.replace(\"Abstract\", \"\",1)\n",
    "\n",
    "            with open(abstracts_path + id + \".txt\", 'w') as f:\n",
    "                f.write(author+\"\\n\"+title+\"\\n\"+abstract)\n",
    "\n",
    "            # Save pdf\n",
    "            pdf_response = requests.get(url + \".pdf\", headers = headers)\n",
    "            with open(pdf_path + id + \".pdf\", 'wb') as f:\n",
    "                f.write(pdf_response.content)\n",
    "\n",
    "            print(id, title)\n",
    "            print(title, url)\n",
    "            print()\n",
    "\n",
    "        # Ignore entries that do not contain a url\n",
    "        except KeyError as e:\n",
    "            #print(\"Entry does not have URL\")\n",
    "            pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extracting PDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to run our analyses on the full texts. However, it is not easy to extract texts from pdfs if you do not want to buy commercial software. \n",
    "\n",
    "Try out the code for extracting texts from pdfs below. For the moment, you can ignore the warnings. Currently, the code outputs only a part of the first file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util_pdf import convert_pdf_to_txt\n",
    "import os\n",
    "\n",
    "for pdf_file in os.listdir(pdf_path):\n",
    "    pdf = os.path.join(pdf_path,pdf_file)\n",
    "    print(pdf)\n",
    "    text = convert_pdf_to_txt(pdf)\n",
    "    print(text[0:10000])\n",
    "    print(\"\\n\\n\")\n",
    "    break\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove the \"break\" command and save the files as txt-files instead. Inspect the quality and discuss for which tasks this quality could be used.**\n",
    "\n",
    "One of the articles throws an error. What could be the reason?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LaDKernel",
   "language": "python",
   "name": "ladkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
