{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright: Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2.2: Basic Language Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the HLT course, you learned how to perform natural language processing steps using the libraries *nltk* and *spacy* in [Lab 1](https://github.com/cltl/ma-hlt-labs/tree/master/lab1.toolkits). Now is a good time to refresh your memory.  \n",
    "Spacy and NLTK are only available for a few languages. In this lab, we will work with *stanza* which is available for more than 60 languages. Take a look at the [documentation](https://stanfordnlp.github.io/stanza/) for details. Stanza is optimized for accuracy and not for speed, so processing takes longer than with spacy. \n",
    "\n",
    "You are free to choose any of the libraries for your assignments. Just make sure to document your selection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "277402db9788470e895838f52dfe5a12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-11 13:45:30 INFO: Downloading default packages for language: fr (French) ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fbb8961e4af4ddebfddde20cdf73130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-fr/resolve/v1.6.0/models/default.zip:   0%|          | 0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-11 13:46:17 INFO: Finished downloading models and saved to /Users/lisabeinborn/stanza_resources.\n",
      "2023-10-11 13:46:17 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b603114c3c12442a87d702f0ccb88f8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-11 13:46:19 INFO: Loading these models for language: fr (French):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "| depparse  | combined_charlm   |\n",
      "| ner       | wikiner           |\n",
      "=================================\n",
      "\n",
      "2023-10-11 13:46:19 INFO: Using device: cpu\n",
      "2023-10-11 13:46:19 INFO: Loading: tokenize\n",
      "2023-10-11 13:46:19 INFO: Loading: mwt\n",
      "2023-10-11 13:46:19 INFO: Loading: pos\n",
      "2023-10-11 13:46:19 INFO: Loading: lemma\n",
      "2023-10-11 13:46:19 INFO: Loading: depparse\n",
      "2023-10-11 13:46:19 INFO: Loading: ner\n",
      "2023-10-11 13:46:20 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import stanza\n",
    "\n",
    "# Read in the data \n",
    "language = \"fr\"\n",
    "article_file = \"../data/veganism_overview_\" + language +\".tsv\"\n",
    "content = pd.read_csv(article_file, sep=\"\\t\", header = 0, keep_default_na=False)\n",
    "\n",
    "# Prepare the nlp pipeline\n",
    "stanza.download(language)\n",
    "nlp = stanza.Pipeline(language)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have loaded the pipeline and can start processing our content. In the example, we only use the first article. **Once you understand how it works, modify the code to iterate through all articles and save the result.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the first article\n",
    "current_article = content[\"Text\"][0]\n",
    "processed_article = nlp(current_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stanza pipeline detects sentence boundaries and segments the texts into tokens. **Analyze the output and check the tokenization quality.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  Les véganeries vont de plus en plus loin.\n",
      "Tokens: \n",
      "1 Les\n",
      "2 véganeries\n",
      "3 vont\n",
      "4 de\n",
      "5 plus\n",
      "6 en\n",
      "7 plus\n",
      "8 loin\n",
      "9 .\n",
      "\n",
      "Sentence:  Connaissez-vous le \"faux mage\" à base de lait de noix de cajou ? ... Article sans intérêt.\n",
      "Tokens: \n",
      "1 Connaissez\n",
      "2 -vous\n",
      "3 le\n",
      "4 \"\n",
      "5 faux\n",
      "6 mage\n",
      "7 \"\n",
      "8 à\n",
      "9 base\n",
      "10 de\n",
      "11 lait\n",
      "12 de\n",
      "13 noix\n",
      "14 de\n",
      "15 cajou\n",
      "16 ?\n",
      "17 ...\n",
      "18 Article\n",
      "19 sans\n",
      "20 intérêt\n",
      "21 .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = processed_article.sentences\n",
    "\n",
    "# For the example, I only look at the first three sentences. Make sure to change this. \n",
    "for sentence in sentences[0:2]:\n",
    "    print(\"Sentence: \", sentence.text)\n",
    "    print(\"Tokens: \")\n",
    "    for token in sentence.tokens:\n",
    "        print(token.id[0], token.text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Token Frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's count the token frequencies in this article. If necessary, go back to [chapter 10](https://github.com/cltl/python-for-text-analysis/blob/master/Chapters/Chapter%2010%20-%20Dictionaries.ipynb) of the python course and refresh your skills on how to count with dictionaries and counters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('de', 34), (',', 31), ('.', 18), ('la', 18), ('des', 15), ('...', 13), ('et', 13), ('en', 12), ('?', 12), ('à', 11), ('pas', 10), ('le', 9), (\"l'\", 9), ('produits', 9), ('pour', 9), ('les', 9), (\"d'\", 7), ('est', 7), ('a', 6), ('vous', 6), ('qui', 6), ('ne', 6), ('ils', 6), ('plus', 5), ('lait', 5), ('cajou', 5), ('alors', 5), ('ça', 5), ('ou', 5), ('notre', 5), ('viande', 5), ('nous', 5), ('mais', 5), ('\"', 4), ('noix', 4), ('y', 4), ('soja', 4), ('un', 4), ('une', 4), (\"n'\", 4)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "token_frequencies = Counter()\n",
    "\n",
    "for sentence in sentences:\n",
    "    all_tokens =[token.text for token in sentence.tokens]\n",
    "    token_frequencies.update(all_tokens)\n",
    "    \n",
    "# Print the most common tokens\n",
    "print(token_frequencies.most_common(40))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The type-token ratio is an indicator for lexical variation. **Think about example texts with very high or very low type-token ratio.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334 738\n",
      "0.4526\n"
     ]
    }
   ],
   "source": [
    "# Type-token ratio\n",
    "num_types = len(token_frequencies.keys())\n",
    "num_tokens = sum(token_frequencies.values())\n",
    "\n",
    "tt_ratio = num_types/num_tokens\n",
    "\n",
    "print(num_types, num_tokens)\n",
    "\n",
    "# Print the type token ratio with 4 decimals\n",
    "print(\"%.4f\" % tt_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Saving as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save complex Python objects like dictionaries (or like the processed articles) in a *pickle* file. This can be convenient if you are running a processing step that takes a lot of time and you want to do it only once. You can then save the intermediate output in a pickle-file and load it directly when you continue working on it. \n",
    "\n",
    "Note that pickle files can also be used to hide harmful code. So make sure to only open pickle files if you know who created them and what they contain. More information can be found in this [tutorial](https://www.datacamp.com/community/tutorials/pickle-python-tutorial).\n",
    "\n",
    "When opening files, *w* stands for write, *r* for read and *b* indicates that the file is binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "frequency_file = \"../data/processed_data/tokenfrequencies_article1.pkl\"\n",
    "pickle.dump(token_frequencies, open(frequency_file, \"wb\"))\n",
    "\n",
    "stanza_objects_file = \"../data/processed_data/nlp_article1.pkl\"\n",
    "pickle.dump(processed_article, open(stanza_objects_file, \"wb\"))\n",
    "\n",
    "# You can then later load the file like this: \n",
    "# loaded_frequencies = pickle.load(open(frequencyfile, \"rb\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Token frequencies of all articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example, we only used the first article. **Once you understand how it works, modify the code to iterate through all articles and save the result in \"../data/processed_data/tokenfrequencies.pkl\".** \n",
    "\n",
    "Stanza processing takes quite long, so you can try it directly with your own dataset, if you prefer. Do not save all stanza objects, the file will get quite big. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables\n",
    "\n",
    "\n",
    "# Iterate through all articles\n",
    "    \n",
    "    # Skip empty articles\n",
    "    \n",
    "    # Process the article with the stanza pipeline\n",
    "    \n",
    "    # Iterate through all sentences of the article\n",
    "    \n",
    "        # Add the tokens to a counter\n",
    "\n",
    "# Save the token frequencies as a pickle file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. More linguistic processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline provides a large amount of additional information. Let's check the representation of the sentence. **Try to understand which information is represented by the attributes *lemma*, *upos*, *feats*, *heads*, *deprel* and *ner*.** We will learn more about this in the next week, but you can already collect some statistics over the information.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentences[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LaD2023",
   "language": "python",
   "name": "lad2023"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
