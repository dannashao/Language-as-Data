{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright: Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2.3: Linguistic Units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will take a closer look how to distinguish between words. We use the processed article from the previous lab. **Modify the code to work with all articles from your dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "import pickle\n",
    "\n",
    "processed_article_file = \"../data/processed_data/nlp_article1.pkl\"\n",
    "nlp_output = pickle.load(open(processed_article_file,\"rb\"))\n",
    "print(nlp_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tokens vs Lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the HLT course, you already learned about the difference between tokens and lemmas. Let's take a look at the difference. \n",
    "\n",
    "It depends on the language you work with and on your analysis goal whether you are more interested in tokens or in lemmas. **Think about some examples.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sentence in enumerate(nlp_output.sentences):\n",
    "    # Only check first 20 sentences\n",
    "    if i==20:\n",
    "        break\n",
    "        \n",
    "    print(str(i), sentence.text)\n",
    "    for word in sentence.words:\n",
    "        if not word.text == word.lemma:\n",
    "            print(word.id, word.text, word.lemma)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Testing Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example, we see that in sentence 19, the lemma for \"VEULENT\" is \"vevler\". The correct lemma should be \"vouloir\". \n",
    "\n",
    "A reason for this mistake might be that the word is written in all-caps. Let's check this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's write a function for testing a single sentence. \n",
    "def get_lemmas(input, stanza_pipeline): \n",
    "    # This is a quite complex list comprehension. Make sure you understand what it does. \n",
    "    lemmas = [word.lemma for word in stanza_pipeline(input).sentences[0].words]\n",
    "    return lemmas\n",
    "\n",
    "# We use a faster pipeline that does not perform all processing steps, only tokenization, POS-tagging and lemmatization\n",
    "french_pipeline = stanza.Pipeline('fr', processors='tokenize,pos,lemma')\n",
    "test1 = 'Ils veulent fabriquer quelque chose.'\n",
    "test2 = 'Ils VEULENT fabriquer quelque chose.'\n",
    "\n",
    "print(get_lemmas(test1, french_pipeline))\n",
    "print(get_lemmas(test2, french_pipeline))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test the lemmatization quality for your own datasets. Collect the tricky cases in this [document](https://docs.google.com/document/d/1tU7KD-WrwYAieMH_Q-6z69NFleTJwp8zIUpCr_rbwlA/edit?usp=sharing)**\n",
    "\n",
    "Sometimes the problem lies already in the tokenization. Do you also find cases for incorrect tokenization? \n",
    "\n",
    "If you find many inconsistencies, you can compare the quality to the output of the nltk or spacy lemmatizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Adding exceptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stanza lemmatizer uses a combination of a dictionary and a neural model. The lemma for any word that cannot be found in the dictionary is approximated by the neural model. Combining several resources is called an ensemble model. \n",
    " \n",
    "We can customize the dictionary to add our own solutions. Check the [documentation](https://stanfordnlp.github.io/stanza/lemma.html#accessing-lemma-for-word).  \n",
    "\n",
    "**Important: If you modify the pipeline like this, you need to be very transparent in your documentation and provide the modified model (or the code to obtain it) to ensure reproducibility!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from os.path import expanduser\n",
    "\n",
    "# Load the current dictionaries\n",
    "home = expanduser(\"~\")\n",
    "# if this is not working, double-check which dictionary is used by your version of stanza\n",
    "model = torch.load(home +'/stanza_resources/fr/lemma/gsd.pt', map_location='cpu')\n",
    "word_dict = model['dicts'][0]\n",
    "\n",
    "# Add a word to the dictionary\n",
    "word_dict['VEULENT'] = 'vouloir'\n",
    "\n",
    "# Save the modified model under a different name\n",
    "torch.save(model, home + '/stanza_resources/fr/lemma/gsd_customized.pt')\n",
    "\n",
    "# Load your customized pipeline\n",
    "customized_pipeline = stanza.Pipeline('fr', package='gsd', processors='tokenize,pos,lemma', lemma_model_path=home + '/stanza_resources/fr/lemma/gsd_customized.pt')\n",
    "test = 'Ils VEULENT fabriquer quelque chose.'\n",
    "print(get_lemmas(test, customized_pipeline))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. POS-tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same lemma can occur in different word classes. For example, *run* can be a verb or a noun. When calculating word frequencies, you might want to distinguish between different POS-tags.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "token_pos_frequencies = Counter()\n",
    "for sentence in nlp_output.sentences:\n",
    "    # Here you could also use word.text instead of word.lemma. Test if it makes a difference!\n",
    "    token_pos = [(word.lemma, word.pos) for word in sentence.words]\n",
    "    token_pos_frequencies.update(token_pos)\n",
    "    \n",
    "print(token_pos_frequencies.most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most frequent words are stopwords. For some research questions, it might make sense to ignore the stopwords.\n",
    "\n",
    "**Search for the commonly used stopwords for your target language. Discuss the role of stopwords for your dataset. **\n",
    "\n",
    "Do you see a difference in the most frequent tokens if you ignore stopwords?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "# These are the stopwords defined for French in the nltk module and I added the determiners \"d'\" and \"l'\"\n",
    "stopwords = ['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', \"d'\",'elle', 'en', 'et', 'eux', 'il', 'ils', 'je', 'la', 'le', 'les',\"l'\", 'leur', 'lui', 'ma', 'mais', 'me', 'même', 'mes', 'moi', 'mon', 'ne', 'nos', 'notre', 'nous', 'on', 'ou', 'par', 'pas', 'pour', 'qu', 'que', 'qui', 'sa', 'se', 'ses', 'son', 'sur', 'ta', 'te', 'tes', 'toi', 'ton', 'tu', 'un', 'une', 'vos', 'votre', 'vous', 'c', 'd', 'j', 'l', 'à', 'm', 'n', 's', 't', 'y', 'été', 'étée', 'étées', 'étés', 'étant', 'étante', 'étants', 'étantes', 'suis', 'es', 'est', 'sommes', 'êtes', 'sont', 'serai', 'seras', 'sera', 'serons', 'serez', 'seront', 'serais', 'serait', 'serions', 'seriez', 'seraient', 'étais', 'était', 'étions', 'étiez', 'étaient', 'fus', 'fut', 'fûmes', 'fûtes', 'furent', 'sois', 'soit', 'soyons', 'soyez', 'soient', 'fusse', 'fusses', 'fût', 'fussions', 'fussiez', 'fussent', 'ayant', 'ayante', 'ayantes', 'ayants', 'eu', 'eue', 'eues', 'eus', 'ai', 'as', 'avons', 'avez', 'ont', 'aurai', 'auras', 'aura', 'aurons', 'aurez', 'auront', 'aurais', 'aurait', 'aurions', 'auriez', 'auraient', 'avais', 'avait', 'avions', 'aviez', 'avaient', 'eut', 'eûmes', 'eûtes', 'eurent', 'aie', 'aies', 'ait', 'ayons', 'ayez', 'aient', 'eusse', 'eusses', 'eût', 'eussions', 'eussiez', 'eussent']\n",
    "\n",
    "def calculate_token_frequencies(nlp_output, ignore_stopwords=False):\n",
    "    token_frequencies = Counter()\n",
    "    for sentence in nlp_output.sentences:\n",
    "        if ignore_stopwords:\n",
    "        # Take some time to understand the syntax of the list comprehension, for ignoring stopwords.\n",
    "        # It is not intuitive\n",
    "            tokens = [token.text for token in sentence.tokens if token.text not in stopwords ]\n",
    "        else:\n",
    "            tokens = [token.text for token in sentence.tokens]\n",
    "\n",
    "        token_frequencies.update(tokens)\n",
    "    return token_frequencies\n",
    "\n",
    "token_frequencies = calculate_token_frequencies(nlp_output, ignore_stopwords=False)\n",
    "print(token_frequencies.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to determine the relative importance of a term for an article, we can normalize its frequency by the frequency of the term in all articles. \n",
    "\n",
    "The code currently distinguishes between uppercase and lowercase words. For many languages and tasks, it is useful to lowercase all words. **Think about the influence of casing on your research question.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code currently throws a *ZeroDivisionError*. **What does that mean and how can you fix it?** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies_currentarticle = calculate_token_frequencies(nlp_output)\n",
    "\n",
    "# You calculated the document frequencies in the previous lab\n",
    "frequencies_dataset = pickle.load(open(\"../data/processed_data/tokenfrequencies.pkl\",\"rb\"))\n",
    "\n",
    "normalized_frequencies = Counter()\n",
    "for token, freq in frequencies_currentarticle.items():\n",
    "    # Remove stopwords and punctuation? --> experimental choice\n",
    "    if not token in stopwords and not token in string.punctuation:\n",
    "        normalized_frequency = float(freq/frequencies_dataset[token])\n",
    "        normalized_frequencies[token] = normalized_frequency\n",
    "    \n",
    "print(normalized_frequencies.most_common(100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LaD2023",
   "language": "python",
   "name": "lad2023"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
